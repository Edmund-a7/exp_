# IAM Agent Inference Configuration
# Based on interactive_inference.yaml with IAM-specific settings

# Architecture
denoising_step_list:
- 1000
- 750
- 500
- 250
warp_denoising_step: true
num_frame_per_block: 3
model_name: Wan2.1-T2V-1.3B
model_kwargs:
  local_attn_size: 12
  timestep_shift: 5.0
  sink_size: 3
  bank_size: 3
  record_interval: 3
  SMA: True  # 启用稀疏注意力 (Sink保留, Bank+Local稀疏化)
  sparse_top_k: 6  # 从 Bank+Local 中选择 top-6 帧

# Inference
data_path: prompts/interactive_example.jsonl
output_folder: videos/iam_interactive
inference_iter: -1
num_output_frames: 240
use_ema: false
seed: 1
num_samples: 1
save_with_index: true
switch_frame_indices: 40, 80, 120, 160, 200
global_sink: true
context_noise: 0

# Checkpoints
generator_ckpt: checkpoints/base.pt
lora_ckpt: checkpoints/lora.pt

# LoRA Adapter
adapter:
  type: "lora"
  rank: 256
  alpha: 256
  dropout: 0.0
  dtype: "bfloat16"
  verbose: false

# IAM Agent Settings (can be overridden via command line)
llm_model_path: ../Qwen3-0.6B
max_memory_frames: 3
save_dir: data/agent_frames
use_vllm: true  # 使用 vLLM 加速 LLM 推理 (需要安装 vllm)
