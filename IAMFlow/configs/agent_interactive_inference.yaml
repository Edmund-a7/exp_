# IAM Agent Inference Configuration
# Based on interactive_inference.yaml with IAM-specific settings

# Architecture
denoising_step_list:
- 1000
- 750
- 500
- 250
warp_denoising_step: true
num_frame_per_block: 3
model_name: Wan2.1-T2V-1.3B
model_kwargs:
  local_attn_size: 12
  timestep_shift: 5.0
  sink_size: 3
  bank_size: 3
  record_interval: 3
  SMA: False  # Disabled when using TCAT
  # TCAT: Temporal Coverage Aware Top-k
  tcat_enabled: True
  tcat_sink_k: 1   # Select top-1 from Sink region (3 chunks)
  tcat_mem_k: 2    # Select top-2 from Mem region (3 chunks)
  tcat_local_k: 3  # Select top-3 from Local-far region (6 chunks)
  tcat_local_near: 3  # Preserve last 3 chunks (most recent frames)
  tcat_ema_alpha: 0.3  # EMA smoothing factor (0=disabled)

# Inference
data_path: prompts/interactive_example.jsonl
output_folder: videos/iam_interactive
inference_iter: -1
num_output_frames: 240
use_ema: false
seed: 1
num_samples: 1
save_with_index: true
switch_frame_indices: 40, 80, 120, 160, 200
global_sink: true
context_noise: 0

# SPT: Soft Prompt Transition
spt_enabled: false  # Set to true to enable soft transition (disables recache)
spt:
  scheduler_type: cosine  # linear, cosine, sigmoid, step
  window_frames: 9        # Transition window (3 chunks)
  delay_frames: 3         # Delay before blending starts (1 chunk)
  steepness: 6.0          # Sigmoid scheduler only
  frames_per_chunk: 3     # Step scheduler only

# Checkpoints
generator_ckpt: checkpoints/base.pt
lora_ckpt: checkpoints/lora.pt

# LoRA Adapter
adapter:
  type: "lora"
  rank: 256
  alpha: 256
  dropout: 0.0
  dtype: "bfloat16"
  verbose: false

# IAM Agent Settings (can be overridden via command line)
llm_model_path: ../Qwen3-0.6B
max_memory_frames: 3
save_dir: data/agent_frames
use_vllm: true  # 使用 vLLM 加速 LLM 推理 (需要安装 vllm)
